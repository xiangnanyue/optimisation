{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instead of\n",
    "plt.plot(np.log(obj_ista - obj_ista[-1]), label=\"penalization %s\" % (s))\n",
    "# use plt.semilogy()\n",
    "\n",
    "# in\n",
    "plt.plot([x for x in range(n_iter+2)],[y for y in objectives_fista],label='FISTA')\n",
    "# The two list comprehensions are useless. \n",
    "# What you are basically doing is plot(list(range(n_iter + 2), objectives_fista)\n",
    "\n",
    "\n",
    "# Use \n",
    "plt.ylabel(\"foo\")\n",
    "plt.xlabel(\"bar\")\n",
    "plt.legend()\n",
    "plt.title(\"dozo\")\n",
    "# to get explicit figures.\n",
    "\n",
    "\n",
    "# labels saying \"ista 0.10\" are not helpful. Say which parameter is equal to 0.10 ??\n",
    "\n",
    "\n",
    "# don't use line plots to plot a vector of coefficients : it makes no sense to \n",
    "# have a line passing through the coefficients values.\n",
    "# and similarly don't use \n",
    "plt.stem(obj)\n",
    "# to plot the obj as a function of the iterations\n",
    "\n",
    "\n",
    "\n",
    "# simpler than:\n",
    "ax.semilogy(np.arange(len(err1)), err1, label=\"ISTA error\")\n",
    "# you could just do:\n",
    "ax.semilogy(err1, label=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in:\n",
    "(A_lin, b_lin) = simu_linreg(coefs, n_samples, corr=0.5)\n",
    "[Alin , blin] = simu_linreg(coefs)\n",
    "# no need for parenthesis of the left hand side. No need for brackets either.\n",
    "\n",
    "# rather than \n",
    "1/float(len(b1))\n",
    "# do \n",
    "1. / len(b1)\n",
    "# to specify than one of them is a float and float division should be used\n",
    "\n",
    "\n",
    "# extra parenthesis make the code more,difficult to read, e.g. in\n",
    "x_new = prox_g(x - step*(grad_f(x)), s*step)\n",
    "# or\n",
    "return (np.linalg.norm(A,ord=2)**2)/(n_samples)\n",
    "# or\n",
    "return 0.5*(1/(n_samples))*(np.linalg.norm(A.dot(x) -b)**2)  \n",
    "# or\n",
    "x_new=prox_g(x-grad_f(x)*(1/step),s*(1/step))\n",
    "\n",
    "\n",
    "# Try and avoid names like tmp, tmp1, tmp2, as in \n",
    "temp = np.exp(-b*np.dot(A,x))\n",
    "temp2= temp/(1+temp)*b\n",
    "return (-1./A.shape[0])*np.sum(A.T*temp2, axis=1)\n",
    "# (also, use PEP8)\n",
    "\n",
    "# No need to align equal signs as was done here:\n",
    "x_new = prox_g(z - step * grad_f(z), s*step)\n",
    "t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2\n",
    "z     = x_new + ((t - 1) / t_new) * (x_new - x)\n",
    "\n",
    "# in\n",
    "n_iter = int(100)  \n",
    "# calling int() is useless. 100 is aalready an int (100. is a float)\n",
    "\n",
    "\n",
    "# No need  to jump lines all the time as in:\n",
    "for k in range(n_iter + 1):\n",
    "    S+=1\n",
    "    x_new = prox_g(z - step*grad_f(z),s,step)\n",
    "\n",
    "    t_new = (1. + np.sqrt(1. + 4.*(t**2)))/2\n",
    "\n",
    "    z = x_new + ((t-1)/t_new)*(x_new-x)\n",
    "\n",
    "    x = x_new\n",
    "\n",
    "    t = t_new\n",
    "# but jump two lines between function definitions\n",
    "\n",
    "\n",
    "# comments are much appreciated, as in:\n",
    "x_new[:] = prox_g(x-step*grad_f(x),step*s,t=1.) # here step = 1/L\n",
    "# good, only missing spaces around binary operators\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instead of storing your useless variables in a variable named dummy \n",
    "# use _ it's a convention in python development.Ex:\n",
    "_, a, _ = function_returning_3_values()\n",
    "\n",
    "\n",
    "# no semicolons, it's unnecesary except if you want to put multiple statements on the same line:\n",
    "nb_iter=100;\n",
    "# is bad\n",
    "\n",
    "\n",
    "# avoid using variables for nothing\n",
    "# e.g. instead of:\n",
    "def foo(s, x):\n",
    "    lasso_value = s*np.linalg.norm(x,1)\n",
    "    return lasso_value\n",
    "\n",
    "# (no need to define lasso_value), just do:\n",
    "def lasso_penalty(s, x):\n",
    "    return s * np.linalg.norm(x, ord=1)\n",
    "\n",
    "# same in:\n",
    "norme_x = linalg.norm(x, 1)\n",
    "g = s*norme_x\n",
    "return g\n",
    "# (seen many times)\n",
    "\n",
    "# same here (for ISTA)\n",
    "for k in range(n_iter + 1):\n",
    "    y = prox_g(x-grad_f(x, A, b)/step, s/step)\n",
    "    x = y\n",
    "# and y is never used after. proofread your code once it works\n",
    "\n",
    "\n",
    "# put your imports at the top of the file so it's clear for the reader what you're using\n",
    "# if you use numpy it's often useless to import math\n",
    "\n",
    "\n",
    "# python does type promotion by itself: in the following, no need to cast s as float\n",
    "s = float(s)\n",
    "a = s*np.linalg.norm(x,ord=1)\n",
    "\n",
    "\n",
    "\n",
    "# be consistent:\n",
    "beta_old = beta_new\n",
    "x[:] = x_new[:]\n",
    "# why beta_old but not x_old ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PEP8: what is the easiest to read between\n",
    "loss_linreg_value = 1/(2*n)*LA.norm(b-np.dot(A, x),2)**2\n",
    "# and\n",
    "loss_linreg_value = 1. / (2 * n) * LA.norm(b - np.dot(A, x), ord=2) ** 2\n",
    "\n",
    "# so put spaces around operators (=, +, *...), \n",
    "# and write short lines (less than 80 characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# never ever ever use :\n",
    "from numpy import transpose as T\n",
    "T(A)\n",
    "# single letter global names are evil, impossible for \n",
    "# somebody reading you to underestand where it comes from\n",
    "# instead, use \n",
    "A.T\n",
    "# also don't use np.transpose(A), it's too long.\n",
    "\n",
    "\n",
    "# avoid for loops: instead of\n",
    "n =len(b)\n",
    "somme =0\n",
    "for i in range(n):\n",
    "    somme += b[i]*LA.norm(A[i])**2\n",
    "L = somme/n\n",
    "return L\n",
    "# use: \n",
    "return (b * np.linalg.norm(A, ord=2, axis=1) ** 2).mean()\n",
    "\n",
    "# this\n",
    "np.linalg.norm(np.transpose(A), axis=0)\n",
    "# is just \n",
    "np.linalg.norm(A, axis=1)\n",
    "\n",
    "\n",
    "# in :\n",
    "temp=np.ones(n)\n",
    "return (1/n)*temp.dot(np.log(1+np.exp(-b*A.dot(x))))\n",
    "# taking dot with a vector of ones is also known as summing\n",
    "\n",
    "\n",
    "# np.dot can operate on 2 1D array, no need to transpose one of them as in\n",
    "np.dot(np.transpose(A[i, :]), x)\n",
    "np.dot(np.ones(5), np.ones(5))  # this works\n",
    "\n",
    "\n",
    "np.vdot # this is for complex valued arrays, or multidimensional array. use np.dot()\n",
    "\n",
    "\n",
    "# most numpy functions work directly on array. avoid\n",
    "sum((-b[i]*np.transpose(A[i, :]) * np.exp(-b[i]*np.dot(np.transpose(A[i, :]), x))) / (len(b) *\n",
    "        (1 + np.exp(-b[i]*np.dot(np.transpose(A[i, :]), x)))) for i in range(len(b)))\n",
    "# 1) the built in function sum() is slower than np.sum()\n",
    "# 2) the line is too long and difficult to read (so error prone)\n",
    "# 3) this can be formulated with a numpy calls directly on A and b\n",
    "\n",
    "\n",
    "# in\n",
    "def prox_ridge(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the ridge at x with strength t\"\"\"\n",
    "    g = np.copy(x)\n",
    "    return g/(1. + t * s)\n",
    "# copying is useless: x / 2 creates a new array anyway. \n",
    "# note that x /= 2 would do the operation inplace.\n",
    "\n",
    "\n",
    "# It's useless to declare     \n",
    "tA = np.transpose(A1)\n",
    "# because transpose does not create a new array anyway. It's just a view on the original data,\n",
    "# the array is not copied\n",
    "# try:\n",
    "a = np.ones([2, 3])\n",
    "b = a.T\n",
    "b[0, 1] = 1970\n",
    "print(a)\n",
    "\n",
    "\n",
    "# WARNING: on 2 arrays norm is the fro norm by default.\n",
    "\n",
    "\n",
    "# try to avoid this in the soft thresholding\n",
    "g[g > t * s] = g[g >  t * s] - t * s \n",
    "# but if you must, do\n",
    "g[g > t * s] -= t * s \n",
    "# to perform the operation in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# don't use \n",
    "np.square(np.linalg.norm(x)) \n",
    "np.absolute(x)\n",
    "float('inf')\n",
    "np.shape(A)[0]\n",
    "# but \n",
    "np.linalg.norm(x) ** 2\n",
    "np.abs(x) # shorter\n",
    "np.inf\n",
    "A.shape[0]\n",
    "\n",
    "\n",
    "# this may comes from matlab but looks extremely unpythonic\n",
    "return (x-s)*(s<x) + (x+s)*(x<-s) \n",
    "\n",
    "\n",
    "# the following is inefficient:\n",
    "def prox_lasso(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the Lasso at x with strength t\"\"\"\n",
    "    s = t*s\n",
    "    return np.array([xi+s if xi < -s else xi-s if xi > s else 0 for xi in x])\n",
    "# list comprehension are super slow and numpy built in functions are generally\n",
    "# super fast to operate on whole arrays in vectorized fashion.\n",
    "# np.maximum is helpful\n",
    "\n",
    "\n",
    "\n",
    "return (b-A@x).T @ (b-A@x) /(2*n)\n",
    "# computes A@x twice which is useless. norm(b - A@x) ** 2 / (2. * n)\n",
    "\n",
    "\n",
    "\n",
    "# in the following line:\n",
    "np.maximum((np.absolute(x) - s * t), np.zeros(x.size))  \n",
    "# np.zeros(x.size) can be replaced by 0 because np will do broadcasting \n",
    "# if you plan on using numpy on a regular basis, read more about broadcasting, \n",
    "# it's an essential feature\n",
    "\n",
    "\n",
    "# Don't use np.matrix. It's mostly here for historical reasons. \n",
    "# It will confuse people because * is not the same operation on matrices and 2 arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_value = s*(1/2)*LA.norm(x,2)**2\n",
    "# will be 0 in python 2 because 1/2 is 0. Use explicit float division: 1. / 2\n",
    "# similarly:\n",
    "corr = [n / 10 for n in range(11)] \n",
    "# is dangerous in python2. prefer np.linspace(0, 1, 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lipschitz constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to compute the spectral norm of a matrix, instead of:\n",
    "def lip_linreg(A):\n",
    "    \"\"\"Lipschitz constant for linear squares loss\"\"\"\n",
    "    U, s, V = np.linalg.svd(np.dot(A.T,A))\n",
    "    return s[0] / n_samples\n",
    "\n",
    "# use:\n",
    "def lip_linreg(A):\n",
    "    \"\"\"Lipschitz constant for linear squares loss\"\"\"\n",
    "    return np.linalg.norm(A, ord=2) ** 2\n",
    "# no need to do full SVD, no need to compute np.dot(A.T, A) either"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when you write the following:\n",
    "L = np.linspace(0, 2.5, 50)\n",
    "\n",
    "for corr in L:\n",
    "    A_lin, b_lin = simu_linreg(coefs, n_samples=1000, corr=corr)\n",
    "\n",
    "# 1) L is not a very explicit variable names, prefer correlations or all_correlations or corr_grid\n",
    "# 2) How can variables have a correlation greater than 1? You get warning when simulating\n",
    "#   A_lin because the covariance matrix you give to random_multivariate is not positive definite..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lots of people got ISTA wrong the correct formula is:\n",
    "x = prox_g(x - step * grad_f(x), s, t=step)\n",
    "# or\n",
    "x = prox_g(x - step * grad_f(x), s * step)\n",
    "# or use step = 1. / L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FISTA and ISTA are used to solve the same problem, so they should converge to the same results.\n",
    "- ISTA is a descent algorithm. If your objective does not decrease there's something wrong.\n",
    "- If your objective after 1000 iterations for FISTA is higher than at iteration 1, there's also a problem\n",
    "- Plotting the objective value instead of the distance to the minimizer makes a less powerful impact, since we can't distinguish between 0.001 to optimum and 0.000001\n",
    "- You are expected to write a bit more text in your answer. Fista distance to minimum seems to make bumps, very few people comment on it.\n",
    "\n",
    "\n",
    "- Many got confused : coefs is NOT the minimizer of the objective functions (because of the regularizer). If your distance to the minimizer/minimum (of the objective function) doesn't go to 0, there is an obvious problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things read somewhere:\n",
    "- \"when the coefficients are decorrelated there is more information\"\n",
    "> What does more information mean?  try to compute the condition number/Lips for const various corr values instead.\n",
    "\n",
    "- \"We also understand that the common hypothesis that the samples are independent and identically distributed is fundamental : the less the correlation, the less the final error.\"\n",
    "> no, the samples are iid, following N(0, Sigma) with sigma toeplitz like.\n",
    "\n",
    "- \"When s is large, the weight associated with the ridge penalization is important, thus the gradient descent algorithm almost comes down to minimizing only this penalization : that is why the algorithm does not converge to the minimizer.\"\n",
    "> not true: if coded correctly, both algorithms converge to the minimizer of the objective...\n",
    "\n",
    "- \"We see that the speed of algorithms is the same for Ridge and Lasso (Lasso is a little bit better in the case of logistic regression, but hardly). It seems normal because the expressions of prox-ridge and prox-lasso are easy to compute, there is no reason that Ridge would be faster than Lasso, or the inverse.\"\n",
    "> we are not talking about timing here, so the cost of the computation does not matter...\n",
    "\n",
    "- \"it has the ability to zero out coefficients thus the Lasso's name \"\n",
    "> ?\n",
    "\n",
    "\n",
    "- \"With the Ridge proximity operator the coefficients have the same shape than the original parameters (there is no coefficient set to zero) but some of them are shrinked\"\n",
    "> no, all of them are shrunk, and by the same factor\n",
    "\n",
    "- \"Ridge gives a better approximation of the minimizer than Lasso. It is logical since lasso will not only do a shrinkage of the coefficients but also set some of them to zero, meaning that we won't be able to approximate the minimizer as precisely as with the ridge method\" \n",
    "> a bit weird, because the coefs vector HAS a sparse structure. So one could expect the Lasso to give better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST BUT NOT LEAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# duplicated code is error prone and difficult to maintain\n",
    "# The following could be 5 lines of code with a for loop, e.g:\n",
    "# for corr in [0.2, 0.5, 0.8]: # do stuff\n",
    "# but instead I often read:\n",
    "\n",
    "\n",
    "A, b = simu_linreg(coefs, n_samples=200, corr=0.2)\n",
    "step= 1/lip_linreg(A)\n",
    "Constante_lin.append(lip_linreg(A))\n",
    "low_r = ista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=s, n_iter =200)\n",
    "low_l = ista(x0, loss_linreg, grad_linreg, lasso, prox_lasso, step, s=s, n_iter =200)\n",
    "\n",
    "A, b = simu_linreg(coefs, n_samples=200, corr=0.5)\n",
    "step= 1/lip_linreg(A)\n",
    "Constante_lin.append(lip_linreg(A))\n",
    "med_r = ista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=s, n_iter =200)\n",
    "med_l = ista(x0, loss_linreg, grad_linreg, lasso, prox_lasso, step, s=s, n_iter =200)\n",
    "\n",
    "A, b = simu_linreg(coefs, n_samples=200, corr=0.8)\n",
    "step= 1/lip_linreg(A)\n",
    "Constante_lin.append(lip_linreg(A))\n",
    "high_r = ista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=s, n_iter =200)\n",
    "high_l = ista(x0, loss_linreg, grad_linreg, lasso, prox_lasso, step, s=s, n_iter =200)\n",
    "\n",
    "\n",
    "lin1f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=0, n_iter =200)\n",
    "lin2f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=0.02, n_iter =200)\n",
    "lin3f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=0.1, n_iter =200)\n",
    "lin4f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=0.25, n_iter =200)\n",
    "lin5f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=0.8, n_iter =200)\n",
    "lin6f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=1, n_iter =200)\n",
    "lin7f = fista(x0, loss_linreg, grad_linreg, ridge, prox_ridge, step, s=5, n_iter =200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, avoid this. For loops are your friend:\n",
    "# This kind of code is difficult to read, difficult to write, difficult to maintain.\n",
    "# From the Zen of Python: \"Beautiful is better than ugly\"\n",
    "corr_1 = 0.2\n",
    "corr_2 = 0.3\n",
    "corr_3 = 0.8\n",
    "corr_4 = -0.3\n",
    "corr_5 = -0.5\n",
    "corr_6 = -0.8\n",
    "\n",
    "n_iter = 70\n",
    "A_lin_corr1,b_lin_corr1=simu_linreg(coefs, n_samples=1000, corr=corr_1)\n",
    "A_lin_corr2,b_lin_corr2=simu_linreg(coefs, n_samples=1000, corr=corr_2)\n",
    "A_lin_corr3,b_lin_corr3=simu_linreg(coefs, n_samples=1000, corr=corr_3)\n",
    "A_lin_corr4,b_lin_corr4=simu_linreg(coefs, n_samples=1000, corr=corr_4)\n",
    "A_lin_corr5,b_lin_corr5=simu_linreg(coefs, n_samples=1000, corr=corr_5)\n",
    "A_lin_corr6,b_lin_corr6=simu_linreg(coefs, n_samples=1000, corr=corr_6)\n",
    "A_log_corr1,b_log_corr1=simu_logreg(coefs, n_samples=1000, corr=corr_1)\n",
    "A_log_corr2,b_log_corr2=simu_logreg(coefs, n_samples=1000, corr=corr_2)\n",
    "A_log_corr3,b_log_corr3=simu_logreg(coefs, n_samples=1000, corr=corr_3)\n",
    "A_log_corr4,b_log_corr4=simu_logreg(coefs, n_samples=1000, corr=corr_4)\n",
    "A_log_corr5,b_log_corr5=simu_logreg(coefs, n_samples=1000, corr=corr_5)\n",
    "A_log_corr6,b_log_corr6=simu_logreg(coefs, n_samples=1000, corr=corr_6)\n",
    "\n",
    "#corr_1\n",
    "def loss_linreg_corr1(x):\n",
    "    return loss_linreg(x, A_lin_corr1, b_lin_corr1)\n",
    "def grad_linreg_corr1(x):\n",
    "    return grad_linreg(x, A_lin_corr1, b_lin_corr1)\n",
    "def loss_logreg_corr1(x):\n",
    "    return loss_linreg(x, A_log_corr1, b_log_corr1)\n",
    "def grad_logreg_corr1(x):\n",
    "    return grad_logreg(x, A_log_corr1, b_log_corr1)\n",
    "\n",
    "#corr_2\n",
    "def loss_linreg_corr2(x):\n",
    "    return loss_linreg(x, A_lin_corr2, b_lin_corr2)\n",
    "def grad_linreg_corr2(x):\n",
    "    return grad_linreg(x, A_lin_corr2, b_lin_corr2)\n",
    "def loss_logreg_corr2(x):\n",
    "    return loss_linreg(x, A_log_corr2, b_log_corr2)\n",
    "def grad_logreg_corr2(x):\n",
    "    return grad_logreg(x, A_log_corr2, b_log_corr2)\n",
    "\n",
    "#corr_3\n",
    "def loss_linreg_corr3(x):\n",
    "    return loss_linreg(x, A_lin_corr3, b_lin_corr3)\n",
    "def grad_linreg_corr3(x):\n",
    "    return grad_linreg(x, A_lin_corr3, b_lin_corr3)\n",
    "def loss_logreg_corr3(x):\n",
    "    return loss_linreg(x, A_log_corr3, b_log_corr3)\n",
    "def grad_logreg_corr3(x):\n",
    "    return grad_logreg(x, A_log_corr3, b_log_corr3)\n",
    "\n",
    "#corr_4\n",
    "def loss_linreg_corr4(x):\n",
    "    return loss_linreg(x, A_lin_corr4, b_lin_corr4)\n",
    "def grad_linreg_corr4(x):\n",
    "    return grad_linreg(x, A_lin_corr4, b_lin_corr4)\n",
    "def loss_logreg_corr4(x):\n",
    "    return loss_linreg(x, A_log_corr4, b_log_corr4)\n",
    "def grad_logreg_corr4(x):\n",
    "    return grad_logreg(x, A_log_corr4, b_log_corr4)\n",
    "\n",
    "#corr_5\n",
    "def loss_linreg_corr5(x):\n",
    "    return loss_linreg(x, A_lin_corr5, b_lin_corr5)\n",
    "def grad_linreg_corr5(x):\n",
    "    return grad_linreg(x, A_lin_corr5, b_lin_corr5)\n",
    "def loss_logreg_corr5(x):\n",
    "    return loss_linreg(x, A_log_corr5, b_log_corr5)\n",
    "def grad_logreg_corr5(x):\n",
    "    return grad_logreg(x, A_log_corr5, b_log_corr5)\n",
    "\n",
    "#corr_6\n",
    "def loss_linreg_corr6(x):\n",
    "    return loss_linreg(x, A_lin_corr6, b_lin_corr6)\n",
    "def grad_linreg_corr6(x):\n",
    "    return grad_linreg(x, A_lin_corr6, b_lin_corr6)\n",
    "def loss_logreg_corr6(x):\n",
    "    return loss_linreg(x, A_log_corr6, b_log_corr6)\n",
    "def grad_logreg_corr6(x):\n",
    "    return grad_logreg(x, A_log_corr6, b_log_corr6)\n",
    "\n",
    "x_min_lin_corr1=fista(x0,loss_linreg_corr1,grad_linreg_corr1,ridge,prox_ridge,step=lip_linreg(A_lin_corr1),s=s,n_iter=1000,verbose = False)[0]\n",
    "x_min_log_corr1=fista(x0,loss_logreg_corr1,grad_logreg_corr1,ridge,prox_ridge,step=lip_logreg(A_log_corr1),s=s,n_iter=1000, verbose = False)[0]\n",
    "x_min_lin_corr2=fista(x0,loss_linreg_corr2,grad_linreg_corr2,ridge,prox_ridge,step=lip_linreg(A_lin_corr2),s=s,n_iter=1000,verbose = False)[0]\n",
    "x_min_log_corr2=fista(x0,loss_logreg_corr2,grad_logreg_corr2,ridge,prox_ridge,step=lip_logreg(A_log_corr2),s=s,n_iter=1000, verbose = False)[0]\n",
    "x_min_lin_corr3=fista(x0,loss_linreg_corr3,grad_linreg_corr3,ridge,prox_ridge,step=lip_linreg(A_lin_corr3),s=s,n_iter=1000,verbose = False)[0]\n",
    "x_min_log_corr3=fista(x0,loss_logreg_corr3,grad_logreg_corr3,ridge,prox_ridge,step=lip_logreg(A_log_corr3),s=s,n_iter=1000, verbose = False)[0]\n",
    "x_min_lin_corr4=fista(x0,loss_linreg_corr4,grad_linreg_corr4,ridge,prox_ridge,step=lip_linreg(A_lin_corr4),s=s,n_iter=1000,verbose = False)[0]\n",
    "x_min_log_corr4=fista(x0,loss_logreg_corr4,grad_logreg_corr4,ridge,prox_ridge,step=lip_logreg(A_log_corr4),s=s,n_iter=1000, verbose = False)[0]\n",
    "x_min_lin_corr5=fista(x0,loss_linreg_corr5,grad_linreg_corr5,ridge,prox_ridge,step=lip_linreg(A_lin_corr5),s=s,n_iter=1000,verbose = False)[0]\n",
    "x_min_log_corr5=fista(x0,loss_logreg_corr5,grad_logreg_corr5,ridge,prox_ridge,step=lip_logreg(A_log_corr5),s=s,n_iter=1000, verbose = False)[0]\n",
    "x_min_lin_corr6=fista(x0,loss_linreg_corr6,grad_linreg_corr6,ridge,prox_ridge,step=lip_linreg(A_lin_corr6),s=s,n_iter=1000,verbose = False)[0]\n",
    "x_min_log_corr6=fista(x0,loss_logreg_corr6,grad_logreg_corr6,ridge,prox_ridge,step=lip_logreg(A_log_corr6),s=s,n_iter=1000, verbose = False)[0]\n",
    "\n",
    "#corr_1\n",
    "err_ista_lin_corr1 = ista(x0,loss_linreg_corr1,grad_linreg_corr1,ridge,prox_ridge,step=lip_linreg(A_lin_corr1),s=s,n_iter=n_iter,x_true=x_min_lin_corr1,verbose = False)[2]\n",
    "err_fista_lin_corr1 = fista(x0,loss_linreg_corr1,grad_linreg_corr1,ridge,prox_ridge,step=lip_linreg(A_lin_corr1),s=s,n_iter=n_iter,x_true=x_min_lin_corr1, verbose = False)[2]\n",
    "\n",
    "err_ista_log_corr1 = ista(x0,loss_logreg_corr1,grad_logreg_corr1,ridge,prox_ridge,step=lip_logreg(A_log_corr1),s=s,n_iter=n_iter,x_true=x_min_log_corr1, verbose = False)[2]\n",
    "err_fista_log_corr1 = fista(x0,loss_logreg_corr1,grad_logreg_corr1,ridge,prox_ridge,step=lip_logreg(A_log_corr1),s=s,n_iter=n_iter,x_true=x_min_log_corr1, verbose = False)[2]\n",
    "\n",
    "#corr_2\n",
    "err_ista_lin_corr2 = ista(x0,loss_linreg_corr2,grad_linreg_corr2,ridge,prox_ridge,step=lip_linreg(A_lin_corr2),s=s,n_iter=n_iter,x_true=x_min_lin_corr2,verbose = False)[2]\n",
    "err_fista_lin_corr2 = fista(x0,loss_linreg_corr2,grad_linreg_corr2,ridge,prox_ridge,step=lip_linreg(A_lin_corr2),s=s,n_iter=n_iter,x_true=x_min_lin_corr2, verbose = False)[2]\n",
    "\n",
    "err_ista_log_corr2 = ista(x0,loss_logreg_corr2,grad_logreg_corr2,ridge,prox_ridge,step=lip_logreg(A_log_corr2),s=s,n_iter=n_iter,x_true=x_min_log_corr2, verbose = False)[2]\n",
    "err_fista_log_corr2 = fista(x0,loss_logreg_corr2,grad_logreg_corr2,ridge,prox_ridge,step=lip_logreg(A_log_corr2),s=s,n_iter=n_iter,x_true=x_min_log_corr2, verbose = False)[2]\n",
    "\n",
    "#corr_3\n",
    "err_ista_lin_corr3 = ista(x0,loss_linreg_corr3,grad_linreg_corr3,ridge,prox_ridge,step=lip_linreg(A_lin_corr3),s=s,n_iter=n_iter,x_true=x_min_lin_corr3,verbose = False)[2]\n",
    "err_fista_lin_corr3 = fista(x0,loss_linreg_corr3,grad_linreg_corr3,ridge,prox_ridge,step=lip_linreg(A_lin_corr3),s=s,n_iter=n_iter,x_true=x_min_lin_corr3, verbose = False)[2]\n",
    "\n",
    "err_ista_log_corr3 = ista(x0,loss_logreg_corr3,grad_logreg_corr3,ridge,prox_ridge,step=lip_logreg(A_log_corr3),s=s,n_iter=n_iter,x_true=x_min_log_corr3, verbose = False)[2]\n",
    "err_fista_log_corr3 = fista(x0,loss_logreg_corr3,grad_logreg_corr3,ridge,prox_ridge,step=lip_logreg(A_log_corr3),s=s,n_iter=n_iter,x_true=x_min_log_corr3, verbose = False)[2]\n",
    "\n",
    "#corr_4\n",
    "err_ista_lin_corr4 = ista(x0,loss_linreg_corr4,grad_linreg_corr4,ridge,prox_ridge,step=lip_linreg(A_lin_corr4),s=s,n_iter=n_iter,x_true=x_min_lin_corr4,verbose = False)[2]\n",
    "err_fista_lin_corr4 = fista(x0,loss_linreg_corr4,grad_linreg_corr4,ridge,prox_ridge,step=lip_linreg(A_lin_corr4),s=s,n_iter=n_iter,x_true=x_min_lin_corr4, verbose = False)[2]\n",
    "\n",
    "err_ista_log_corr4 = ista(x0,loss_logreg_corr4,grad_logreg_corr4,ridge,prox_ridge,step=lip_logreg(A_log_corr4),s=s,n_iter=n_iter,x_true=x_min_log_corr4, verbose = False)[2]\n",
    "err_fista_log_corr4 = fista(x0,loss_logreg_corr4,grad_logreg_corr4,ridge,prox_ridge,step=lip_logreg(A_log_corr4),s=s,n_iter=n_iter,x_true=x_min_log_corr4, verbose = False)[2]\n",
    "\n",
    "#corr_5\n",
    "err_ista_lin_corr5 = ista(x0,loss_linreg_corr5,grad_linreg_corr5,ridge,prox_ridge,step=lip_linreg(A_lin_corr5),s=s,n_iter=n_iter,x_true=x_min_lin_corr5,verbose = False)[2]\n",
    "err_fista_lin_corr5 = fista(x0,loss_linreg_corr5,grad_linreg_corr5,ridge,prox_ridge,step=lip_linreg(A_lin_corr5),s=s,n_iter=n_iter,x_true=x_min_lin_corr5, verbose = False)[2]\n",
    "\n",
    "err_ista_log_corr5 = ista(x0,loss_logreg_corr5,grad_logreg_corr5,ridge,prox_ridge,step=lip_logreg(A_log_corr5),s=s,n_iter=n_iter,x_true=x_min_log_corr5, verbose = False)[2]\n",
    "err_fista_log_corr5 = fista(x0,loss_logreg_corr5,grad_logreg_corr5,ridge,prox_ridge,step=lip_logreg(A_log_corr5),s=s,n_iter=n_iter,x_true=x_min_log_corr5, verbose = False)[2]\n",
    "\n",
    "#corr_6\n",
    "err_ista_lin_corr6 = ista(x0,loss_linreg_corr6,grad_linreg_corr6,ridge,prox_ridge,step=lip_linreg(A_lin_corr6),s=s,n_iter=n_iter,x_true=x_min_lin_corr6,verbose = False)[2]\n",
    "err_fista_lin_corr6 = fista(x0,loss_linreg_corr6,grad_linreg_corr6,ridge,prox_ridge,step=lip_linreg(A_lin_corr6),s=s,n_iter=n_iter,x_true=x_min_lin_corr6, verbose = False)[2]\n",
    "\n",
    "err_ista_log_corr6 = ista(x0,loss_logreg_corr6,grad_logreg_corr6,ridge,prox_ridge,step=lip_logreg(A_log_corr6),s=s,n_iter=n_iter,x_true=x_min_log_corr6, verbose = False)[2]\n",
    "err_fista_log_corr6 = fista(x0,loss_logreg_corr6,grad_logreg_corr6,ridge,prox_ridge,step=lip_logreg(A_log_corr6),s=s,n_iter=n_iter,x_true=x_min_log_corr6, verbose = False)[2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(4, 3)\n",
    "\n",
    "f.set_figheight(20)\n",
    "f.set_figwidth(20)\n",
    "\n",
    "plt_ista = axarr[0,0].semilogy(err_ista_lin_corr1, label =\"ISTA\", color='orange', linewidth=2)\n",
    "plt_fista = axarr[0,0].semilogy(err_fista_lin_corr1, label =\"FISTA\", linewidth=2)\n",
    "axarr[0,0].set_title('linear regression, correlation = 0.2')\n",
    "axarr[0,0].set_xlabel('Iteration')\n",
    "axarr[0,0].set_ylabel('Error')\n",
    "axarr[0,0].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[1,0].semilogy(err_ista_log_corr1, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[1,0].semilogy(err_fista_log_corr1, label =\"FSTA\", linewidth=2)\n",
    "axarr[1,0].set_title('logistic regression, correlation = 0.2')\n",
    "axarr[1,0].set_xlabel('Iteration')\n",
    "axarr[1,0].set_ylabel('Error')\n",
    "axarr[1,0].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[0,1].semilogy(err_ista_lin_corr2, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[0,1].semilogy(err_fista_lin_corr2, label =\"FSTA\", linewidth=2)\n",
    "axarr[0,1].set_title('linear regression, correlation = 0.5')\n",
    "axarr[0,1].set_xlabel('Iteration')\n",
    "axarr[0,1].set_ylabel('Error')\n",
    "axarr[0,1].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[1,1].semilogy(err_ista_log_corr2, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[1,1].semilogy(err_fista_log_corr2, label =\"FSTA\", linewidth=2)\n",
    "axarr[1,1].set_title('logistic regression, correlation = 0.5')\n",
    "axarr[1,1].set_xlabel('Iteration')\n",
    "axarr[1,1].set_ylabel('Error')\n",
    "axarr[1,1].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[0,2].semilogy(err_ista_lin_corr3, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[0,2].semilogy(err_fista_lin_corr3, label =\"FSTA\", linewidth=2)\n",
    "axarr[0,2].set_title('linear regression, correlation = 0.8')\n",
    "axarr[0,2].set_xlabel('Iteration')\n",
    "axarr[0,2].set_ylabel('Error')\n",
    "axarr[0,2].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[1,2].semilogy(err_ista_log_corr3, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[1,2].semilogy(err_fista_log_corr3, label =\"FSTA\", linewidth=2)\n",
    "axarr[1,2].set_title('logistic regression, correlation = 0.8')\n",
    "axarr[1,2].set_xlabel('Iteration')\n",
    "axarr[1,2].set_ylabel('Error')\n",
    "axarr[1,2].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[2,0].semilogy(err_ista_lin_corr4, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[2,0].semilogy(err_fista_lin_corr4, label =\"FISTA\", linewidth=2)\n",
    "axarr[2,0].set_title('linear regression, correlation = -0.2')\n",
    "axarr[2,0].set_xlabel('Iteration')\n",
    "axarr[2,0].set_ylabel('Error')\n",
    "axarr[2,0].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[3,0].semilogy(err_ista_log_corr4, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[3,0].semilogy(err_fista_log_corr4, label =\"FSTA\", linewidth=2)\n",
    "axarr[3,0].set_title('logistic regression, correlation = -0.2')\n",
    "axarr[3,0].set_xlabel('Iteration')\n",
    "axarr[3,0].set_ylabel('Error')\n",
    "axarr[3,0].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[2,1].semilogy(err_ista_lin_corr5, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[2,1].semilogy(err_fista_lin_corr5, label =\"FSTA\", linewidth=2)\n",
    "axarr[2,1].set_title('linear regression, correlation = -0.5')\n",
    "axarr[2,1].set_xlabel('Iteration')\n",
    "axarr[2,1].set_ylabel('Error')\n",
    "axarr[2,1].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[3,1].semilogy(err_ista_log_corr5, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[3,1].semilogy(err_fista_log_corr5, label =\"FSTA\", linewidth=2)\n",
    "axarr[3,1].set_title('logistic regression, correlation = -0.5')\n",
    "axarr[3,1].set_xlabel('Iteration')\n",
    "axarr[3,1].set_ylabel('Error')\n",
    "axarr[3,1].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[2,2].semilogy(err_ista_lin_corr6, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[2,2].semilogy(err_fista_lin_corr6, label =\"FSTA\", linewidth=2)\n",
    "axarr[2,2].set_title('linear regression, correlation = -0.8')\n",
    "axarr[2,2].set_xlabel('Iteration')\n",
    "axarr[2,2].set_ylabel('Error')\n",
    "axarr[2,2].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])\n",
    "\n",
    "axarr[3,2].semilogy(err_ista_log_corr6, label =\"ISTA\", color='orange', linewidth=2)\n",
    "axarr[3,2].semilogy(err_fista_log_corr6, label =\"FSTA\", linewidth=2)\n",
    "axarr[3,2].set_title('logistic regression, correlation = -0.8')\n",
    "axarr[3,2].set_xlabel('Iteration')\n",
    "axarr[3,2].set_ylabel('Error')\n",
    "axarr[3,2].legend(handles=[plt_ista[0],plt_fista[0]],labels=['ISTA','FISTA'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
