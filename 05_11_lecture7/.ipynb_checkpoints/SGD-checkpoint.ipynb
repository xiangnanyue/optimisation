{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: Stochastic Gradient Descent\n",
    "\n",
    "Author : Alexandre Gramfort, Robert Gower\n",
    "\n",
    "The objective of this lab session is to implement:\n",
    "- Stochastic gradient descent with constant stepsizes\n",
    "- Stochastic gradient descent with shrinking stepsizes\n",
    "- Stochastic gradient descent with sampling with/without replacement\n",
    "- Stochastic gradient descent with averaging \n",
    "\n",
    "and compare your implementation with gradient descent.\n",
    "\n",
    "Exceptionally, this lab is not worth any points. Solutions will be presented at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "## 1. Loss functions, gradients and step-sizes\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n \\ell(x_i^\\top \\theta, y_i) + \\frac \\lambda 2 \\|\\theta\\|_2^2\n",
    "$$\n",
    "where\n",
    "- $\\ell(z, y) = \\frac 12 (y - z)^2$ (least-squares regression)\n",
    "- $\\ell(z, y) = \\log(1 + \\exp(-yz))$ (logistic regression).\n",
    "\n",
    "We write it as a a minimization problem of the form\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n f_i(\\theta)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(\\theta) = \\ell(x_i^\\top \\theta, y_i) + \\frac \\lambda 2 \\|\\theta\\|_2^2.\n",
    "$$\n",
    "\n",
    "For both cases, the gradients are\n",
    "$$\n",
    "\\nabla f_i(\\theta) = (x_i^\\top \\theta - y_i) x_i + \\lambda \\theta\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla f_i(\\theta) = - \\frac{y_i}{1 + \\exp(y_i x_i^\\top \\theta)} x_i + \\lambda \\theta.\n",
    "$$\n",
    "\n",
    "Denote by $L$ the Lipschitz constant of $f$.\n",
    "One can see easily that for linear regression\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X^\\top \\mathbf X \\|_{2}}{n} + \\lambda \n",
    "$$\n",
    "while for logistic regression it is\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf X^\\top \\mathbf X \\|_{2}}{4 n} + \\lambda \n",
    "$$\n",
    "For full-gradient methods, the theoretical step-size is $1 / L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce a class that will be used for the solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "class LinReg(object):\n",
    "    \"\"\"A class for the least-squares regression with\n",
    "    Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    \n",
    "    def grad(self, x):\n",
    "        return self.A.T.dot(self.A.dot(x) - b) / self.n + self.lbda * x\n",
    "\n",
    "    \n",
    "    def f(self, x):\n",
    "        return norm(self.A.dot(x) - b) ** 2 / (2. * self.n) + self.lbda * norm(x) ** 2 / 2.\n",
    "    \n",
    "    \n",
    "    def grad_i(self, i, x):\n",
    "        ####################################\n",
    "        # Compute here the stochastic gradient\n",
    "        # gradi = ...\n",
    "        ####################################\n",
    "        grad_i = []\n",
    "        return grad_i\n",
    "\n",
    "    \n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        return norm(self.A, ord=2) ** 2 / self.n + self.lbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \"\"\"A class for the logistic regression with L2 penalization\"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    \n",
    "    def grad(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        temp = 1. / (1. + np.exp(bAx))\n",
    "        grad = - np.dot(self.A.T, self.b * temp) / self.n + self.lbda * x\n",
    "        return grad\n",
    "\n",
    "    \n",
    "    def f(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        return np.mean(np.log(1. + np.exp(- bAx))) + self.lbda * norm(x) ** 2 / 2.\n",
    "    \n",
    "    \n",
    "    def grad_i(self, i, x):\n",
    "        ####################################\n",
    "        # Compute here the stochastic gradient\n",
    "        # grad_i = ...\n",
    "        ####################################\n",
    "        grad_i =[]\n",
    "        return grad_i\n",
    "\n",
    "    \n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        return norm(self.A, ord=2) ** 2  / (4. * self.n) + self.lbda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 2. Generate a dataset\n",
    "\n",
    "We generate datasets for the least-squares and the logistic cases. First we define a function for the least-squares case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "    \n",
    "def simu_linreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the least-squares problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    d = x.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    A = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    b = A.dot(x) + noise\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simu_logreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation of the logistic regression problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape=(d,)\n",
    "        The coefficients of the model\n",
    "    \n",
    "    n : int\n",
    "        Sample size\n",
    "    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \"\"\"    \n",
    "    A, b = simu_linreg(x, n, std=1., corr=0.5)\n",
    "    return A, np.sign(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 50\n",
    "n = 10000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "x_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "A, b = simu_linreg(x_model_truth, n, std=1., corr=0.1)\n",
    "# A, b = simu_logreg(x_model_truth, n, std=1., corr=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "lbda = 1. / n ** 0.5\n",
    "model = LinReg(A, b, lbda)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.f, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbda = 1. / n ** 0.5\n",
    "model = LogReg(A, b, lbda)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.f, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinReg(A, b, lbda)\n",
    "#model = LogReg(A, b, lbda)\n",
    "xtt = np.random.randn(d)\n",
    "model.grad_i(2, xtt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='constant'></a> \n",
    "\n",
    "## 3. Implementing Stochastic Gradient Descent\n",
    "\n",
    "Complete the code below. The inputs are\n",
    "- n_iter: The number of iterations\n",
    "- indices: an np.array of indices of length n_iter. The indices[k]  is the index of stochastic gradient that will be used on the kth iteration. \n",
    "- steps: an np.array of positive floats of length n_iter. The steps[k] is the stepsize used on the kth iteration.\n",
    "- averaging_on: is a boolean which indicates if the output should be the average of the iterates.\n",
    "\n",
    "The outputs are:\n",
    "- x_output: The final x vector found by the algorithm\n",
    "- objectives: A list containing the sequence function values calculated during the iterations of the algorithm \n",
    "- errors: If x_min is no empty, errors is a list containing the sequence of errors || x - x_min || of the algorithm. Otherwise errors should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(x0,model, indices, steps, x_min, n_iter=100,  averaging_on=False, verbose=True):\n",
    "    \"\"\"Stochastic gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "    # average x\n",
    "    x_average = x0.copy()\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1.0\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error if x_min is not empty\n",
    "    if np.any(x_min):\n",
    "        err = norm(x - x_min) / norm(x_min)\n",
    "        errors.append(err)\n",
    "    # Current objective\n",
    "    obj = model.f(x) \n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching SGD solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter + 1):        \n",
    "        ####################################\n",
    "        # Compute the next iterate\n",
    "        #  x_new[:]  = ........\n",
    "        ####################################\n",
    "        x[:] = x_new\n",
    "        ####################################\n",
    "        # Compute the average iterate \n",
    "        # x_average[:]  = ...x_average + .....\n",
    "        ####################################\n",
    "        \n",
    "        if averaging_on:  \n",
    "            x_test = x_average.copy()  # report progress of average iterate if  averaging_on = True\n",
    "        else:\n",
    "            x_test = x.copy()      \n",
    "        obj = model.f(x_test) \n",
    "        if np.any(x_min):\n",
    "            err = norm(x_test - x_min) / norm(x_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if verbose and k % 1000 == 0:\n",
    "            if np.any(x_min):\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "            else:\n",
    "                print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8)]))\n",
    "    if averaging_on:\n",
    "        x_output = x_average.copy()\n",
    "    else:\n",
    "        x_output = x.copy()    \n",
    "    return x_output, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Setup\n",
    "x0 = np.zeros(d)  # initial iterate\n",
    "datapasses = 2  # number of sweeps through all the data. This means that there will datapasses*n stochastic gradient updates\n",
    "niters = int(datapasses * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Get accurate estimate of solution\n",
    "\n",
    "####################################\n",
    "# Compute an accurate estimate of the solution using many iterations! Hint: about 5*datapasses of SGD is enough or 500 of gradient descent\n",
    "# obj_min  = ...\n",
    "# x_min = ....\n",
    "####################################\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with constant step with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Execute SGD with a constant stepsize. Please name the output as\n",
    "# x_sgdcr, obj_sgdcr, err_sgdcr = sgd(...?....)\n",
    "# HINT: You will have to guess a stepsize and see if it works! Something around 0.005 should work here.\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with $1/\\sqrt t$ stepsizes with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Execute SGD with a shrinking stepsize \\alpha_t = C/\\sqrt(t). Please name the output as\n",
    "# x_sgdsr, obj_sgdsr, err_sgdsr = sgd(.....?.....)\n",
    "# HINT: You will have to guess C and see if it works! Something around C = 0.1 should work here.\n",
    "####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(obj_sgdcr - obj_min, label=\"SGD const\", lw=2)\n",
    "plt.plot(obj_sgdsr - obj_min, label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Error of objective\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(err_sgdcr , label=\"SGD const\", lw=2)\n",
    "plt.plot(err_sgdsr , label=\"SGD shrink\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Compare the solution you obtain for SGD with constant stepsizes and SGD with shrinking stepsizes. \n",
    "- Which one is faster in the beginning? Which reaches the \"best\" solution?\n",
    "- What happens when is you use sampling without replacement instead? Hint: Do only one datapass, it's annoying to adapt this implementation for more than one datapass when sampling without replacement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare with averaging step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Execute SGD with averaging on and shrinking stepsize. Please name the output as\n",
    "# x_sgdar, obj_sgdar, err_sgdar = sgd( .... )\n",
    "# HINT: You can use the stepsize you found in the last step.\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"SGD shrink\", lw=2)\n",
    "plt.semilogy(obj_sgdar - obj_min, label=\"SGD average\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Error of the objective\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimizer on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(err_sgdsr , label=\"SGD shrink\", lw=2)\n",
    "plt.semilogy(err_sgdar , label=\"SGD average\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"#iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Compare the solution you obtain for SGD with shrinking stepsizes and SGD with averaging. \n",
    "- When is averaging useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with gradient descent\n",
    "Compare with gradient descent. Compute how many steps you should take with the gradient descent so that the total computational effort is equal to the total computational effort spent on computing the stochastic gradient descent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gd(x0,model, step, x_min =[], n_iter=100, verbose=True):\n",
    "    \"\"\"Gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    err = 1. \n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    if np.any(x_min):\n",
    "        err = norm(x - x_min) / norm(x_min)\n",
    "        errors.append(err)\n",
    "    # Current objective\n",
    "    obj = model.f(x) \n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print(\"Lauching GD solver...\")\n",
    "        print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter ):\n",
    "        x_new[:] = x - steps[k] * model.grad(x)\n",
    "        x[:] = x_new\n",
    "        obj = model.f(x) \n",
    "        if np.any(x_min):\n",
    "            err = norm(x - x_min) / norm(x_min)\n",
    "            errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if verbose:\n",
    "            print(' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Calculate the number of iterations of GD so that the total computational effort is the same as your execution of SGD.\n",
    "# gditerations = .....\n",
    "# HINT: Each iteration of the GD algorithm calculates n stochastic gradients!\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# Execute the gradient algorithm. Please name the output as\n",
    "# x_gd, obj_gd, err_gd = gd(.....)\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This calculates the total complexity of GD assuming you executed gditerations iterations. This needed for the plot in the next cell.\n",
    "complexityofGD = n*np.arange(0, gditerations + 1)\n",
    "print(complexityofGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Error of objective on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(complexityofGD,obj_gd - obj_min, label=\"gd\", lw=2)\n",
    "plt.semilogy(obj_sgdsr - obj_min, label=\"sgd\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"# SGD iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()\n",
    "# Distance to the minimum on a logarithmic scale\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(complexityofGD,err_gd , label=\"gd\", lw=2)\n",
    "plt.semilogy(err_sgdsr , label=\"sgd\", lw=2)\n",
    "plt.title(\"Convergence plot\", fontsize=16)\n",
    "plt.xlabel(\"# SGD iterations\", fontsize=14)\n",
    "plt.ylabel(\"Distance to the minimum\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Compare the convergence of SGD and GD, what can you conclude?\n",
    "- What happens if you increase the number of datapasses?\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
