{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic and linear regression with deterministic and stochastic first order methods\n",
    "\n",
    "    Lab 3 : Optimisation - DataScience Master\n",
    "    Authors : Robert Gower, Alexandre Gramfort\n",
    "   \n",
    "The aim of this lab is to implement and compare various batch and stochastic algorithms for linear and logistic with ridge penalization. \n",
    "\n",
    "The following methods are compared in this notebook.\n",
    "\n",
    "**Batch (deterministic) methods**\n",
    "\n",
    "- gradient descent (GD)\n",
    "- accelerated gradient descent (AGD)\n",
    "- L-BFGS\n",
    "- conjugate gradient (CG)\n",
    "\n",
    "**Stochastic algorithms**\n",
    "\n",
    "- stochastic gradient descent (SGD)\n",
    "- stochastic averaged gradient (SAG)\n",
    "- stochastic variance reduced gradient (SVRG)\n",
    "\n",
    "Note that we consider as use-cases logistic and linear regression with ridge penalization only, although most of the algorithms below can be used with many other models, and other types of penalization, eventually non-smooth ones, such as the $\\ell_1$ penalization.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 26th of november at 23:55**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"robert\"\n",
    "ln1 = \"gower\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "\n",
    "[1. Loss functions, gradients and step-sizes](#loss)<br>\n",
    "[2. Generate a dataset](#data)<br>\n",
    "[3. Deterministic methods](#batch)<br>\n",
    "[4. Stochastic methods](#stoc)<br>\n",
    "[5. Numerical comparison](#comp)<br>\n",
    "[6. Conclusion](#conc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "## 1. Loss functions, gradients and step-sizes\n",
    "\n",
    "\n",
    "We want to minimize\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n \\ell(a_i^\\top x, b_i) + \\frac \\lambda 2 \\|x\\|_2^2\n",
    "$$\n",
    "where\n",
    "- $\\ell(z, b) = \\frac 12 (b - z)^2$ (least-squares regression)\n",
    "- $\\ell(z, b) = \\log(1 + \\exp(-bz))$ (logistic regression).\n",
    "\n",
    "We write it as a minimization problem of the form\n",
    "$$\n",
    "\\frac 1n \\sum_{i=1}^n f_i(x)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f_i(x) = \\ell(a_i^\\top x, b_i) + \\frac \\lambda 2 \\|x\\|_2^2.\n",
    "$$\n",
    "\n",
    "For both cases, the gradients are\n",
    "$$\n",
    "\\nabla f_i(x) = (a_i^\\top x - b_i) a_i + \\lambda x\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\nabla f_i(x) = - \\frac{b_i}{1 + \\exp(b_i a_i^\\top x)} a_i + \\lambda x.\n",
    "$$\n",
    "\n",
    "Denote by $L$ (resp. $L_i$) the Lipschitz constant of $f$ (resp. $f_i$) and $\\mathbf A^\\top = [a_1, \\ldots, a_n].$\n",
    "One can easily see (using $\\|\\cdot\\|_{2}$ for the matrix spectrale norm) that for linear regression\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf A^\\top \\mathbf A \\|_{2}}{n} + \\lambda \\quad \\text{ and } L_i = \\| a_i \\|_2^2 + \\lambda\n",
    "$$\n",
    "while for logistic regression it is\n",
    "$$\n",
    "L = \\frac{ \\|\\mathbf A^\\top \\mathbf A \\|_{2}}{4 n} + \\lambda \\quad \\text{ and } L_i = \\frac 14 \\| a_i \\|_2^2 + \\lambda.\n",
    "$$\n",
    "For full-gradient methods, the theoretical step-size is $1 / L$, while for SAG and SVRG (see below) it can be taken as\n",
    "$1 / (\\max_{i=1,\\ldots,n} L_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce a class that will be used for the solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "\n",
    "class LinReg(object):\n",
    "    \"\"\"A class for the least-squares regression with L2/Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "\n",
    "    def grad(self, x):\n",
    "        return self.A.T.dot(self.A.dot(x) - self.b) / self.n + self.lbda * x\n",
    "\n",
    "    def loss(self, x):\n",
    "        return norm(self.A.dot(x) - self.b) ** 2 / (2. * self.n) + self.lbda * norm(x) ** 2 / 2.\n",
    "\n",
    "    def grad_i(self, i, x):\n",
    "        a_i = self.A[i]\n",
    "        return (a_i.dot(x) - self.b[i]) * a_i + self.lbda * x\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        return norm(self.A, ord=2) ** 2 / self.n + self.lbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    \"\"\"A class for the logistic regression with L2/Ridge penalization\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, lbda):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.n, self.d = A.shape\n",
    "        self.lbda = lbda\n",
    "    \n",
    "    def grad(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        temp = 1. / (1. + np.exp(bAx))\n",
    "        grad = - np.dot(self.A.T, self.b * temp) / self.n + self.lbda * x\n",
    "        return grad\n",
    "\n",
    "    def loss(self, x):\n",
    "        bAx = self.b * np.dot(self.A, x)\n",
    "        return np.mean(np.log(1. + np.exp(- bAx))) + self.lbda * norm(x) ** 2 / 2.\n",
    "    \n",
    "    def grad_i(self, i, x):\n",
    "        grad = - self.A[i] * self.b[i] / (1. + np.exp(self.b[i] * np.dot(self.A[i], x)))\n",
    "        grad += self.lbda * x\n",
    "        return grad\n",
    "\n",
    "    def lipschitz_constant(self):\n",
    "        \"\"\"Return the Lipschitz constant of the gradient\"\"\"\n",
    "        return norm(self.A, ord=2) ** 2 / (4. * self.n) + self.lbda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 2. Generate a dataset\n",
    "\n",
    "We generate datasets for the least-squares and the logistic cases. First we define a function for the least-squares case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal, randn\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "\n",
    "\n",
    "def simu_linreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation for the least-squares problem\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (d,)\n",
    "        The coefficients of the model\n",
    "    n : int\n",
    "        Sample size\n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : ndarray, shape (n, d)\n",
    "        The design matrix.\n",
    "    b : ndarray, shape (n,)\n",
    "        The targets.\n",
    "    \"\"\"\n",
    "    d = x.shape[0]\n",
    "    cov = toeplitz(corr ** np.arange(0, d))\n",
    "    A = multivariate_normal(np.zeros(d), cov, size=n)\n",
    "    noise = std * randn(n)\n",
    "    b = A.dot(x) + noise\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simu_logreg(x, n, std=1., corr=0.5):\n",
    "    \"\"\"\n",
    "    Simulation for the logistic regression problem\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (d,)\n",
    "        The coefficients of the model\n",
    "    n : int\n",
    "        Sample size    \n",
    "    std : float, default=1.\n",
    "        Standard-deviation of the noise\n",
    "    corr : float, default=0.5\n",
    "        Correlation of the features matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A : ndarray, shape (n, d)\n",
    "        The design matrix.\n",
    "    b : ndarray, shape (n,)\n",
    "        The targets.\n",
    "    \"\"\"    \n",
    "    A, b = simu_linreg(x, n, std=1., corr=0.5)\n",
    "    return A, np.sign(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = 50\n",
    "n = 10000\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients of the model\n",
    "x_model_truth = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "_A, _b = simu_linreg(x_model_truth, n, std=1., corr=0.1)\n",
    "#_A, _b = simu_logreg(x_model_truth, n, std=1., corr=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.stem(x_model_truth);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerically check loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "lbda = 1. / n ** (0.5)\n",
    "model = LinReg(_A, _b, lbda)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.loss, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbda = 1. / n ** (0.5)\n",
    "model = LogReg(_A, _b, lbda)\n",
    "\n",
    "# Check that the gradient and the loss numerically match\n",
    "check_grad(model.loss, model.grad, np.random.randn(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = LinReg(_A, _b, lbda)\n",
    "model = LogReg(_A, _b, lbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the theoretical step-size for FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "step = 1. / model.lipschitz_constant()\n",
    "\n",
    "print(\"step = %s\" % step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a very precise minimum to compute distances to minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "x_init = np.zeros(d)\n",
    "x_min, f_min, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, pgtol=1e-30, factr=1e-30)\n",
    "print(f_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='batch'></a> \n",
    "\n",
    "## 3. Deterministic methods (ISTA, FISTA, BFGS)\n",
    "\n",
    "Before implementing GD, ISTA or FISTA we provide a simple function to be called after each iteration to gather and display a few metrics about the current minimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def inspector(loss_fun, x_real, verbose=False):\n",
    "    \"\"\"A closure called to update metrics after each iteration.\"\"\"\n",
    "    objectives = []\n",
    "    errors = []\n",
    "    t0 = time.time()\n",
    "    it = [0] # This is a hack to be able to modify 'it' inside the closure.\n",
    "    timing = [] \n",
    "    def inspector_cl(xk):\n",
    "        obj = loss_fun(xk) - f_min\n",
    "        err = norm(xk - x_min)\n",
    "        tim = time.time()-t0\n",
    "        objectives.append(obj)\n",
    "        errors.append(err)\n",
    "        timing.append(tim)\n",
    "        if verbose == True:\n",
    "            if it[0] == 0:\n",
    "                print(' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\", \"time\"]]))\n",
    "            if it[0] % (n_iter / 5) == 0:\n",
    "                print(' | '.join([(\"%d\" % it[0]).rjust(8), (\"%.2e\" % obj).rjust(8), (\"%.2e\" % err).rjust(8), (\"%.4e\" % tim).rjust(8)]))\n",
    "            it[0] += 1\n",
    "    inspector_cl.obj = objectives\n",
    "    inspector_cl.err = errors\n",
    "    inspector_cl.timing = timing\n",
    "\n",
    "    return inspector_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISTA\n",
    "\n",
    "We recall that an iteration of ISTA (actually a batch gradient here) writes\n",
    "\n",
    "$$\n",
    "x_{k+1} \\gets x_k - \\eta \\nabla f(x_k)\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the step-size (that can be chosen in theory as $\\eta = 1 / L$, with $L$ the Lipshitz constant of $\\nabla f$, see above)\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the ISTA solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ista(x_init, grad, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"ISTA algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        \n",
    "        ### TODO\n",
    "        x \n",
    "        step\n",
    "        grad(x)\n",
    "        ### END TODO\n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if callback is not None: \n",
    "            callback(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 1. / model.lipschitz_constant()\n",
    "x_init = np.zeros(d)\n",
    "ista_inspector = inspector(model.loss, x_min, verbose=True)\n",
    "x_ista = ista(x_init, model.grad, n_iter, step, callback=ista_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISTA\n",
    "\n",
    "We recall that an iteration of FISTA (actually an accelerated batch gradient here) writes\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_{k+1} &\\gets y_k - \\eta \\nabla f(y_k) \\\\\n",
    "t_{k+1} &\\gets \\frac{1 + \\sqrt{1 + 4 t_k^2}}{2} \\\\\n",
    "y_{k+1} &\\gets x_{k+1} + \\frac{t_k-1}{t_{k+1}} (x_{k+1} - x_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the step-size (that can be chosen in theory as $\\eta = 1 / L$, with $L$ the Lipshitz constant of $\\nabla f$, see above)\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the FISTA solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fista(x_init, grad, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"FISTA algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    y = x_init.copy()\n",
    "    t = 1.\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        \n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "\n",
    "        # Update metrics after each iteration.\n",
    "        if callback is not None: \n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 1. / model.lipschitz_constant()\n",
    "x_init = np.zeros(d)\n",
    "fista_inspector = inspector(model.loss, x_min, verbose=True)\n",
    "x_fista = fista(x_init, model.grad, n_iter, step, callback=fista_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize's conjuguate gradient\n",
    "\n",
    "Let's compare with ``scipy.optimize``'s conjuguate gradient solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Conjugate gradient descent\n",
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "cg_inspector = inspector(model.loss, x_min, verbose=True)\n",
    "x_cg = fmin_cg(model.loss, x_init, model.grad, maxiter=n_iter, callback=cg_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.optimize's BFGS\n",
    "\n",
    "Let's compare with ``scipy.optimize``'s BFGS solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conjugate gradient descent\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "bfgs_inspector = inspector(model.loss, x_min, verbose=True)\n",
    "x_bfgs, _, _ = fmin_l_bfgs_b(model.loss, x_init, model.grad, maxiter=n_iter, callback=bfgs_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first numerical comparison of deterministic solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "inspectors = [ista_inspector, fista_inspector, cg_inspector, bfgs_inspector]\n",
    "\n",
    "solvers = [\"ISTA\", \"FISTA\", \"CG\", \"BFGS\"]\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.semilogy(insp.obj, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"objective\", fontsize=14)\n",
    "\n",
    "plt.legend(solvers)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.semilogy(insp.err, lw=2)\n",
    "    plt.title(\"Distance to optimum\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"$\\|x_k - x^*\\|_2$\", fontsize=14)\n",
    "\n",
    "plt.legend(solvers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "xs = [x_ista, x_fista, x_cg, x_bfgs]\n",
    "\n",
    "for i, name, x in zip(range(1, 5), solvers, xs):\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.stem(x)\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First conclusions\n",
    "\n",
    "*QUESTIONS*:\n",
    "\n",
    "- Give some first conclusions about the batch solver studied here\n",
    "- What do you observe about FISTA? is it suprising ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stoc'></a> \n",
    "## 4. Stochastic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "\n",
    "# generate indices of random samples\n",
    "iis = np.random.randint(0, n, n * n_iter)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD\n",
    "\n",
    "We recall that an iteration of SGD writes\n",
    "\n",
    "- Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "- Apply\n",
    "$$\n",
    "x_{t+1} \\gets x_t - \\frac{\\eta_0}{\\sqrt{t+1}} \\nabla f_i(x_t)\n",
    "$$\n",
    "\n",
    "where $\\eta_0$ is a step-size to be tuned by hand.\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SGD solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(x_init, iis, grad_i, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    \n",
    "    for idx in range(n_iter):\n",
    "        i = iis[idx]\n",
    "        \n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        \n",
    "        # Update metrics after each iteration.\n",
    "        if callback is not None and idx % n == 0:\n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step0 = 1.\n",
    "x_init = np.zeros(d)\n",
    "sgd_inspector = inspector(model.loss, x_min, verbose=True)\n",
    "x_sgd = sgd(x_init, iis, model.grad_i, n * n_iter, step=step0, callback=sgd_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAG\n",
    "\n",
    "We recall that an iteration of SAG writes\n",
    "\n",
    "For $t=1, \\ldots, $ until convergence\n",
    "\n",
    "1. Pick $i_t$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "\n",
    "2. Update the average of gradients\n",
    "$$\n",
    "G_t \\gets \\frac 1n \\sum_{i=1}^n g_i^t\n",
    "$$\n",
    "where \n",
    "$$\n",
    "g_i^t =\n",
    "\\begin{cases}\n",
    "    \\nabla f_{i}(x_t) &\\text{ if } i = i_t \\\\\n",
    "    g_i^{t-1} & \\text{ otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. Apply the step \n",
    "$$x_{t+1} \\gets x_t - \\eta G_t$$\n",
    "where $\\eta$ is the step-size (see code below).\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SAG solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sag(x_init, iis, grad_i, n_iter=100, step=1., callback=None):\n",
    "    \"\"\"Stochastic average gradient algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    # Old gradients\n",
    "    gradient_memory = np.zeros((n, d))\n",
    "    y = np.zeros(d)\n",
    "\n",
    "    for idx in range(n_iter):\n",
    "        i = iis[idx]\n",
    "        \n",
    "        ### TODO\n",
    "\n",
    "        ### END OF TODO\n",
    "        \n",
    "        # Update metrics after each iteration.\n",
    "        if callback is not None and idx % n == 0:\n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_squared_sum = np.max(np.sum(model.A ** 2, axis=1))\n",
    "\n",
    "if isinstance(model, LogReg):\n",
    "    step = 4.0 / (max_squared_sum + 4.0 * model.lbda / model.n)\n",
    "else:\n",
    "    step = 1.0 / (max_squared_sum + model.lbda / model.n)\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "sag_inspector = inspector(model.loss, x_min, verbose=True)\n",
    "x_sag = sag(x_init, iis, model.grad_i, n * n_iter, step, callback=sag_inspector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVRG\n",
    "\n",
    "We recall that an iteration of SVRG writes\n",
    "\n",
    "For $k=1, \\ldots, $ until convergence\n",
    "\n",
    "1. Set $\\tilde x \\gets \\tilde x^{(k)}$ and $x_1^{(k)} \\gets \\tilde x$\n",
    "2. Compute $\\mu_k \\gets \\nabla f(\\tilde x)$\n",
    "3. For $t=1, \\ldots, n$\n",
    "    4. Pick $i$ uniformly at random in $\\{1, \\ldots, n\\}$\n",
    "    5. Apply the step \n",
    "$$\n",
    "x_{t+1}^{(k)} \\gets x_t^{(k)} - \\eta \\big(\\nabla f_{i}(x_t^{(k)}) - \\nabla f_{i}(\\tilde x) + \\mu_k \\big) \n",
    "$$\n",
    "\n",
    "6. Set $\\tilde x^{(k+1)} \\gets x_{n+1}^{(k)}$\n",
    "\n",
    "where $\\eta$ is the step-size (see code below).\n",
    "\n",
    "*QUESTION*:\n",
    "- Fill in the iteration of the SVRG solver in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svrg(x_init, iis, grad, grad_i, n_iter, step, callback=None):\n",
    "    \"\"\"Stochastic variance reduction gradient algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    x_old = x.copy()\n",
    "    \n",
    "    for idx in range(n_iter):\n",
    "        \n",
    "        ### TODO\n",
    "\n",
    "        \n",
    "        ### END TODO        \n",
    "        \n",
    "        # Update metrics after each full pass on data.\n",
    "        if callback is not None and idx % n == 0:\n",
    "            callback(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step = 1. / (20. * model.lipschitz_constant())\n",
    "\n",
    "x_init = np.zeros(d)\n",
    "svrg_inspector = inspector(model.loss, x_min, verbose=True)    \n",
    "x_svrg = svrg(x_init, iis, model.grad, model.grad_i, n * n_iter, step, callback=svrg_inspector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "inspectors = [sgd_inspector, sag_inspector, svrg_inspector]\n",
    "\n",
    "solvers = [\"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.semilogy(insp.obj, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"$f(x_k) - f(x^*)$\", fontsize=14)\n",
    "\n",
    "plt.legend(solvers)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.semilogy(insp.err, lw=2)\n",
    "    plt.title(\"Distance to optimum\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"$\\|x_k - x^*\\|_2$\", fontsize=14)\n",
    "\n",
    "plt.legend(solvers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comp'></a> \n",
    "## 5. Numerical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "inspectors = [ista_inspector, fista_inspector, cg_inspector, bfgs_inspector,\n",
    "              sgd_inspector, sag_inspector, svrg_inspector]\n",
    "\n",
    "solvers = [\"ISTA\", \"FISTA\", \"CG\", \"BFGS\", \"SGD\", \"SAG\", \"SVRG\"]\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.semilogy(insp.obj, lw=2)\n",
    "    plt.title(\"Loss\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"$f(x_k) - f(x^*)$\", fontsize=14)\n",
    "\n",
    "plt.legend(solvers)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for insp in inspectors:\n",
    "    plt.semilogy(insp.err, lw=2)\n",
    "    plt.title(\"Distance to optimum\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=14)\n",
    "    plt.ylabel(\"$\\|x_k - x^*\\|_2$\", fontsize=14)\n",
    "\n",
    "plt.legend(solvers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='conc'></a>\n",
    "## 6. Conclusion\n",
    "\n",
    "*QUESTIONS*:\n",
    "- Compare and comment your results\n",
    "- Change the value of the ridge regularization (the ``lbda`` parameter) to low ridge $\\lambda = 1 / n$ and high ridge regularization $\\lambda = 1 / \\sqrt n$ and compare your results. Comment.\n",
    "- Play also with the level of correlation between features (parameter ``corr`` above), and compare results with low and high correlation.\n",
    "- Conclude"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
